{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dff1a680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5ca35d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import *\n",
    "\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a730447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dd8a5740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import Constant\n",
    "from keras.layers import (LSTM, \n",
    "                          Embedding, \n",
    "                          BatchNormalization,\n",
    "                          Dense, \n",
    "                          TimeDistributed, \n",
    "                          Dropout, \n",
    "                          Bidirectional,\n",
    "                          Flatten, \n",
    "                          GlobalMaxPool1D)\n",
    "#from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb56db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90ecaab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96dfe49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5274b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                            message\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/spam.csv\", encoding = 'latin-1') \n",
    "df = df.dropna(how=\"any\", axis=1)\n",
    "df.columns = ['target', 'message']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83d9d64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['message_len'] = df['message'].apply(lambda x: len(x.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf9e1a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(df['message_len'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb649ea",
   "metadata": {},
   "source": [
    "### Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b695f7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stop words\n",
    "# nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "add_words = ['hehe', 'im', 'c', 'u']\n",
    "stop_words = stop_words + add_words \n",
    "\n",
    "# Define stemmer \n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Set to lower case, remove symbols, numbers, breaks\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n",
    "    \n",
    "    # Apply stemmer\n",
    "    text = ' '.join(stemmer.stem(word) for word in text.split(' '))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92f06499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>message</th>\n",
       "      <th>message_len</th>\n",
       "      <th>message_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>20</td>\n",
       "      <td>go jurong point crazi avail bugi n great world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>6</td>\n",
       "      <td>ok lar joke wif oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>28</td>\n",
       "      <td>free entri  wkli comp win fa cup final tkts  m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>11</td>\n",
       "      <td>dun say earli hor alreadi say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>13</td>\n",
       "      <td>nah dont think goe usf live around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                            message  message_len  \\\n",
       "0    ham  Go until jurong point, crazy.. Available only ...           20   \n",
       "1    ham                      Ok lar... Joking wif u oni...            6   \n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...           28   \n",
       "3    ham  U dun say so early hor... U c already then say...           11   \n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...           13   \n",
       "\n",
       "                                       message_clean  \n",
       "0  go jurong point crazi avail bugi n great world...  \n",
       "1                                ok lar joke wif oni  \n",
       "2  free entri  wkli comp win fa cup final tkts  m...  \n",
       "3                      dun say earli hor alreadi say  \n",
       "4          nah dont think goe usf live around though  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['message_clean'] = df['message'].apply(preprocess_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8e91878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>message</th>\n",
       "      <th>message_len</th>\n",
       "      <th>message_clean</th>\n",
       "      <th>target_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>20</td>\n",
       "      <td>go jurong point crazi avail bugi n great world...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>6</td>\n",
       "      <td>ok lar joke wif oni</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>28</td>\n",
       "      <td>free entri  wkli comp win fa cup final tkts  m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>11</td>\n",
       "      <td>dun say earli hor alreadi say</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>13</td>\n",
       "      <td>nah dont think goe usf live around though</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                            message  message_len  \\\n",
       "0    ham  Go until jurong point, crazy.. Available only ...           20   \n",
       "1    ham                      Ok lar... Joking wif u oni...            6   \n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...           28   \n",
       "3    ham  U dun say so early hor... U c already then say...           11   \n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...           13   \n",
       "\n",
       "                                       message_clean  target_encoded  \n",
       "0  go jurong point crazi avail bugi n great world...               0  \n",
       "1                                ok lar joke wif oni               0  \n",
       "2  free entri  wkli comp win fa cup final tkts  m...               1  \n",
       "3                      dun say earli hor alreadi say               0  \n",
       "4          nah dont think goe usf live around though               0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode target variable\n",
    "le = LabelEncoder()\n",
    "le.fit(df['target'])\n",
    "\n",
    "df['target_encoded'] = le.transform(df['target'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b3c78ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5572 5572\n"
     ]
    }
   ],
   "source": [
    "X = df['message_clean']\n",
    "y = df['target_encoded']\n",
    "\n",
    "print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c9849d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4179 4179\n",
      "1393 1393\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 42)\n",
    "print(len(X_train), len(y_train))\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9114ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_features=100, ngram_range=(1, 2), stop_words='english')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorize\n",
    "vect = CountVectorizer(stop_words='english', ngram_range=(1,2), max_features=100)\n",
    "vect.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d009fa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document-term matrices\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3182423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "tfidf_transformer.fit(X_train_dtm)\n",
    "X_train_tfidf = tfidf_transformer.transform(X_train_dtm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9f74247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4179x100 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9160 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3488efa4",
   "metadata": {},
   "source": [
    "### GloVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "398eaadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6769"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Length of the vocabulary\n",
    "texts = df['message_clean']\n",
    "target = df['target_encoded']\n",
    "\n",
    "\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(texts)\n",
    "\n",
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "vocab_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fc297add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   2, 3199,  276, ...,    0,    0,    0],\n",
       "       [   8,  239,  532, ...,    0,    0,    0],\n",
       "       [   9,  361,  591, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [6767, 1007, 6768, ...,    0,    0,    0],\n",
       "       [ 139, 1257, 1612, ...,    0,    0,    0],\n",
       "       [1998,  382,  171, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad sequences\n",
    "def embed(corpus): \n",
    "    return word_tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "longest_train = max(texts, key=lambda sentence: len(word_tokenize(sentence)))\n",
    "length_long_sentence = len(word_tokenize(longest_train))\n",
    "\n",
    "train_padded_sentences = pad_sequences(\n",
    "    embed(texts), \n",
    "    length_long_sentence, \n",
    "    padding='post'\n",
    ")\n",
    "\n",
    "train_padded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da81b72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4311a12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare embedding dictionary\n",
    "embeddings_dict = {}\n",
    "embedding_dim = 50\n",
    "\n",
    "with open(\"data/glove.6B/glove.6B.50d.txt\") as fp:\n",
    "    for line in fp.readlines():\n",
    "        records = line.split()\n",
    "        word = records[0]\n",
    "        vector_dimensions = np.asarray(records[1:], dtype = 'float32')\n",
    "        embeddings_dict[word] = vector_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "73342a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_length, embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef6330dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.098201  ,  0.39923999,  0.25696999, ...,  0.70283997,\n",
       "         0.32207   ,  0.77503002],\n",
       "       [ 0.14827999,  0.17761   ,  0.42346001, ..., -0.2182    ,\n",
       "         0.12971   ,  0.32953   ],\n",
       "       ...,\n",
       "       [-0.68614   , -0.20372   , -0.12739   , ..., -0.18347   ,\n",
       "         0.54004002,  0.77217001],\n",
       "       [ 0.21509001, -0.2832    ,  0.16023999, ...,  0.15110999,\n",
       "        -0.12344   ,  1.00170004],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word, index in word_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "        \n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d82ea8f",
   "metadata": {},
   "source": [
    "### Naive Bayes, Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "25ea9269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB = MultinomialNB()\n",
    "NB.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4afe3d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = NB.predict(X_test_dtm)\n",
    "y_pred_prob = NB.predict_proba(X_test_dtm)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1b629516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9361091170136396\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3bf99f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1180   22]\n",
      " [  67  124]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3579974d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.96      1202\n",
      "           1       0.85      0.65      0.74       191\n",
      "\n",
      "    accuracy                           0.94      1393\n",
      "   macro avg       0.90      0.82      0.85      1393\n",
      "weighted avg       0.93      0.94      0.93      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2168653e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9340475298586126\n"
     ]
    }
   ],
   "source": [
    "print(metrics.roc_auc_score(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a8c7f5",
   "metadata": {},
   "source": [
    "### Naive Bayes, TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8f622750",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('bow', CountVectorizer()),\n",
    "                     ('tfdif', TfidfTransformer()),\n",
    "                     ('model', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1d9d05bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_pred_prob = pipeline.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5ce67126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      1202\n",
      "           1       1.00      0.71      0.83       191\n",
      "\n",
      "    accuracy                           0.96      1393\n",
      "   macro avg       0.98      0.85      0.90      1393\n",
      "weighted avg       0.96      0.96      0.96      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "27ac92cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9745624656985303\n"
     ]
    }
   ],
   "source": [
    "print(metrics.roc_auc_score(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b1aa53",
   "metadata": {},
   "source": [
    "### XGBoost, TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8d59fd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('bow', CountVectorizer()),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('model',xgb.XGBClassifier(\n",
    "                 learning_rate = 0.1,\n",
    "                 max_depth = 7,\n",
    "                 n_estimators = 100,\n",
    "                 use_label_encoder = False,\n",
    "                 eval_metric = 'auc'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d1c5dd4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('bow', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('model',\n",
       "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=1, enable_categorical=False,\n",
       "                               eval_metric='auc', gamma=0, gpu_id=-1,\n",
       "                               importance_type=None, interaction_constraints='',\n",
       "                               learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
       "                               min_child_weight=1, missing=nan,\n",
       "                               monotone_constraints='()', n_estimators=100,\n",
       "                               n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
       "                               random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                               scale_pos_weight=1, subsample=1,\n",
       "                               tree_method='exact', use_label_encoder=False,\n",
       "                               validate_parameters=1, verbosity=None))])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c7fd381e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)\n",
    "y_pred_prob = pipeline.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "29c721b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      1202\n",
      "           1       0.98      0.77      0.87       191\n",
      "\n",
      "    accuracy                           0.97      1393\n",
      "   macro avg       0.97      0.89      0.92      1393\n",
      "weighted avg       0.97      0.97      0.97      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "545d6151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.977243425007187\n"
     ]
    }
   ],
   "source": [
    "print(metrics.roc_auc_score(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e8962a",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cc7c7614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_padded_sentences, \n",
    "    target, \n",
    "    test_size=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "591ef54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 10:52:18.676937: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 80, 50)            338450    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 80, 160)          83840     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 160)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 160)              640       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 160)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 80)                12880     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 80)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 80)                6480      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 80)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 442,371\n",
      "Trainable params: 442,051\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def glove_lstm():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(\n",
    "        input_dim=embedding_matrix.shape[0], \n",
    "        output_dim=embedding_matrix.shape[1], \n",
    "        weights = [embedding_matrix], \n",
    "        input_length=length_long_sentence\n",
    "    ))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(\n",
    "        length_long_sentence, \n",
    "        return_sequences = True, \n",
    "        recurrent_dropout=0.2\n",
    "    )))\n",
    "    \n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(length_long_sentence, activation = \"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(length_long_sentence, activation = \"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = glove_lstm()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4aad4371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "131/131 [==============================] - ETA: 0s - loss: 0.3508 - accuracy: 0.8658\n",
      "Epoch 1: val_loss improved from inf to 0.29293, saving model to model.h5\n",
      "131/131 [==============================] - 33s 195ms/step - loss: 0.3508 - accuracy: 0.8658 - val_loss: 0.2929 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 2/7\n",
      "131/131 [==============================] - ETA: 0s - loss: 0.1963 - accuracy: 0.9354\n",
      "Epoch 2: val_loss improved from 0.29293 to 0.17593, saving model to model.h5\n",
      "131/131 [==============================] - 26s 197ms/step - loss: 0.1963 - accuracy: 0.9354 - val_loss: 0.1759 - val_accuracy: 0.9620 - lr: 0.0010\n",
      "Epoch 3/7\n",
      "131/131 [==============================] - ETA: 0s - loss: 0.1367 - accuracy: 0.9560\n",
      "Epoch 3: val_loss improved from 0.17593 to 0.13843, saving model to model.h5\n",
      "131/131 [==============================] - 22s 171ms/step - loss: 0.1367 - accuracy: 0.9560 - val_loss: 0.1384 - val_accuracy: 0.9569 - lr: 0.0010\n",
      "Epoch 4/7\n",
      "131/131 [==============================] - ETA: 0s - loss: 0.1109 - accuracy: 0.9655\n",
      "Epoch 4: val_loss improved from 0.13843 to 0.10567, saving model to model.h5\n",
      "131/131 [==============================] - 23s 175ms/step - loss: 0.1109 - accuracy: 0.9655 - val_loss: 0.1057 - val_accuracy: 0.9691 - lr: 0.0010\n",
      "Epoch 5/7\n",
      "131/131 [==============================] - ETA: 0s - loss: 0.1065 - accuracy: 0.9727\n",
      "Epoch 5: val_loss did not improve from 0.10567\n",
      "131/131 [==============================] - 24s 186ms/step - loss: 0.1065 - accuracy: 0.9727 - val_loss: 0.1321 - val_accuracy: 0.9734 - lr: 0.0010\n",
      "Epoch 6/7\n",
      "131/131 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9765\n",
      "Epoch 6: val_loss did not improve from 0.10567\n",
      "131/131 [==============================] - 27s 206ms/step - loss: 0.0741 - accuracy: 0.9765 - val_loss: 0.1552 - val_accuracy: 0.9734 - lr: 0.0010\n",
      "Epoch 7/7\n",
      "131/131 [==============================] - ETA: 0s - loss: 0.0811 - accuracy: 0.9763\n",
      "Epoch 7: val_loss did not improve from 0.10567\n",
      "131/131 [==============================] - 25s 192ms/step - loss: 0.0811 - accuracy: 0.9763 - val_loss: 0.1372 - val_accuracy: 0.9749 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# Load the model and train\n",
    "model = glove_lstm()\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'model.h5', \n",
    "    monitor = 'val_loss', \n",
    "    verbose = 1, \n",
    "    save_best_only = True\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor = 'val_loss', \n",
    "    factor = 0.2, \n",
    "    verbose = 1, \n",
    "    patience = 5,                        \n",
    "    min_lr = 0.001\n",
    ")\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs = 7,\n",
    "    batch_size = 32,\n",
    "    validation_data = (X_test, y_test),\n",
    "    verbose = 1,\n",
    "    callbacks = [reduce_lr, checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d312b51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 1s 22ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      1208\n",
      "           1       0.92      0.89      0.90       185\n",
      "\n",
      "    accuracy                           0.97      1393\n",
      "   macro avg       0.95      0.94      0.94      1393\n",
      "weighted avg       0.97      0.97      0.97      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "y_preds = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "#y_preds = (model.predict(X_test)).astype(\"int32\")\n",
    "\n",
    "\n",
    "print(metrics.classification_report(y_test, y_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
